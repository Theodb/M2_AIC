{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EM Algorithm** (Expectation - Maximization)\n",
    "\n",
    "We're going to implement the EM algorithm for a mixture of Bernoullis\n",
    "\n",
    "The Expectation-Maximization algo. is used in sk-learn, for instance in GMMs: http://scikit-learn.org/stable/modules/mixture.html#estimation-algorithm-expectation-maximization\n",
    "\n",
    "Additional notes are available here: https://allauzen.github.io/articles/MixturesAndEM/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Think for a bit\n",
    "\n",
    "Is EM a supervised or unsupervised learning algorithm ? \n",
    "\n",
    "What kind of data set is MNIST ? \n",
    "\n",
    "Then, what are we going to do ?  How is this called ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implement the EM algo. for a mixture of Bernoulli (laws)\n",
    "- The cluster number K should be an argument of the function\n",
    "- A maximum number of iterations, *MaxIt*, should act as stopping condition\n",
    "- During the E step, compute and *store* the log-likelihood of the data, so as to monitor its evolution along iterations (~epochs)\n",
    "\n",
    "Apply the algorithm on MNIST:\n",
    "- try out K=5,10,15\n",
    "- Visualize the images coresponding to each cluster's paraemeters. Would that be as straightforward in a Gaussian model (visualizing all the model's parameters?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tp1-mnist.pkl.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-967556711bb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m### Load the dataset -- (python3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./tp1-mnist.pkl.gz'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Unpickler\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'latin1'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mbinary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"write\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tp1-mnist.pkl.gz'"
     ]
    }
   ],
   "source": [
    "## we load the whole data set at once, so as to manipulate it directly (using numpy arrays)\n",
    "## for very large data sets, one needs to read on-the-fly (at least at production time, not at the debugging stage)\n",
    "\n",
    "# ## Load the dataset -- (python2)\n",
    "# import cPickle, gzip\n",
    "# with gzip.open('./tp1-mnist.pkl.gz','rb') as f :\n",
    "#     train_set, valid_set, test_set = cPickle.load(f)\n",
    "\n",
    "### Load the dataset -- (python3)\n",
    "import pickle, gzip\n",
    "with gzip.open('./tp1-mnist.pkl.gz','rb') as f :\n",
    "    u = pickle._Unpickler( f )\n",
    "    u.encoding = 'latin1'\n",
    "    train_set, valid_set, test_set = u.load()\n",
    "\n",
    "## split train an dtest data, to avoid inadvertently cheating.\n",
    "unlabelled_dataset = train_set[0].copy()\n",
    "labels_for_final_accuracy_measurement = train_set[1].copy()\n",
    "del train_set\n",
    "\n",
    "## these would be useful, for now we don't use them\n",
    "del valid_set  \n",
    "del test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## binarize so as to go into Bernoulli-space\n",
    "threshold = 0.3\n",
    "for i in range(unlabelled_dataset.shape[0]): \n",
    "    unlabelled_dataset[i,:]= 1.0*(unlabelled_dataset[i]>threshold)\n",
    "## (unlabelled_dataset[i]>threshold) is now an array of Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspect the new values \"by hand\", to check.\n",
    "unlabelled_dataset[0][300:400] ## reading \n",
    "## we do NOT convert to Integers, instead we stay in floats. Because we'll compute averages later (thus, divide..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for debugging, we may want to use a small piece of the sample\n",
    "subSampling = 100  ## data size will be divided by a factor \"subSampling\"\n",
    "## in the end, we may use all of it (set this to 1) \n",
    "\n",
    "## sort data to make sure all classes are equally present ##\n",
    "ordre = np.argsort(labels_for_final_accuracy_measurement)\n",
    "unlabelled_dataset = unlabelled_dataset[ordre]\n",
    "labels_for_final_accuracy_measurement = labels_for_final_accuracy_measurement[ordre]\n",
    "\n",
    "## do the sub-sampling\n",
    "dataset = unlabelled_dataset[::subSampling].copy()\n",
    "subSampledLabels = labels_for_final_accuracy_measurement[::subSampling].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check it:\n",
    "subSampledLabels[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## when you code, start by writing on a copy of the data \n",
    "## (not the data themselves: otherwise on the second run of the jupyter cell, you'll have run twice)\n",
    "K=10\n",
    "MaxIt=4\n",
    "\n",
    "## compute the proba of an image xi, \n",
    "## given its cluster k and parameters theta \n",
    "def compute_P_Xi_given_k_and_theta(xi, k, theta):\n",
    "    ## TODO \n",
    "    return output\n",
    "\n",
    "outputForDebug = compute_P_Xi_given_k_and_theta(xi, k, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python-note: at debug time, don't write a function, instead, write directly in the main\n",
    "## when evreything works well, encapsulate the piece of code into a function for later re-use (and lisibility)\n",
    "#def AlgoEM(dataset, K, MaxIt, labels_for_final_accuracy_measurement):\n",
    "\n",
    "## we set the seed to a constant so as to have repeatable experiences \n",
    "## (at debug time, and when comparing hyper-parameters)\n",
    "np.random.seed(42)\n",
    "\n",
    "## we'll assume dataset[i,j] is the value of pixel j of image i.\n",
    "Nex = (dataset.shape)[0] ## number of examples (= number of images)\n",
    "dim = (dataset.shape)[1] ## dimension of parameters space (=number of pixels per image, in the Bernoulli case)\n",
    "classFrequencies = np.zeros(K) ## denoted pi_k \n",
    "theta = np.zeros((K, dim))   ## denoted theta_{k,j}\n",
    "\n",
    "## initialization ##\n",
    "affectations = np.random.random((K, Nex)) ## denoted a_{k,i}\n",
    "## normalization step (needed to make the )\n",
    "for i in range(Nex):\n",
    "    affectations[:,i] /= np.sum(affectations[:,i])\n",
    "    \n",
    "## main loop ##\n",
    "for iteration in range(0,MaxIt,1):\n",
    "    # if iteration%10 == 0:\n",
    "    #     print(\"iteration numero\"+str(iteration))\n",
    "\n",
    "    ######################\n",
    "    ## step M: update of \"pi_k, mu_k\" (classFrequencies, theta)  ##\n",
    "    ## TODO \n",
    "\n",
    "    ######################\n",
    "    ## step E: update of \"a_ik\" (affectations) ##\n",
    "    ## TODO \n",
    "\n",
    "    ## monitoring of the quality of te clustering ##\n",
    "    LogLikelihood = np.log(np.max(affectations, axis=0)) \n",
    "    print(np.mean(LogLikelihood))\n",
    "    ## TODO: record the monitoring into an array, to be able to later plot it\n",
    "\n",
    "#    return affectations, theta, classFrequencies\n",
    "##########################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classFrequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LogVraisemblance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can visualize the parameters theta, cluster by cluster\n",
    "for k in range(K):\n",
    "    plt.imshow(theta[k].reshape(28,28) , matplotlib.pyplot.cm.jet)\n",
    "    matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: after checking your code works, put it inside a function and call that function \n",
    " (instead of running code in the *main()* directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affectations, theta, classFrequencies = AlgoEM(dataset, K, MaxIt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's monitor something more interpretable than the log-likelihood... what could it be ? \n",
    "\n",
    "Trick: you may use the function np.argmax(), and maybe just a couple of labels from a couple of examples\n",
    "\n",
    "**Quesiton**: would this strategy be possible for a purely unsupervised task ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: (later) \n",
    "## what , 0,1,2,...9\n",
    "# prediction = np.argmax(affectations, axis=0)\n",
    "# score = np.mean(prediction == labels_for_final_accuracy_measurement[:subSampling])\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add this interesting metric into your function, and add some plot of it from the function as well (possibly along wth the log-likelihood plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you may play with the hyper-parameters.\n",
    "\n",
    "What is the best value of the main hyper-parameter ? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
