{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Lab-exercises-3\">Lab exercises 3<a class=\"anchor-link\" href=\"#Lab-exercises-3\">¶</a></h1><p>For this lab exercises, please submit 2 notebooks / python script and 2 reports, one for each part. The deadline is 22 december.</p>\n",
    "<p>It is important the you <strong>read the documentation</strong> to understand how to use Pytorch functions, what kind of transformation they apply etc. You have to <strong>take time to read it carefully</strong> to understand what you are doing.</p>\n",
    "<ul>\n",
    "<li><a href=\"https://pytorch.org/docs/stable/nn.html\">https://pytorch.org/docs/stable/nn.html</a></li>\n",
    "<li><a href=\"https://pytorch.org/docs/stable/torch.html\">https://pytorch.org/docs/stable/torch.html</a></li>\n",
    "</ul>\n",
    "<h2 id=\"1.-Part-one:-MNIST-classification-with-Pytorch\">1. Part one: MNIST classification with Pytorch<a class=\"anchor-link\" href=\"#1.-Part-one:-MNIST-classification-with-Pytorch\">¶</a></h2><p>The goal of the first part is to learn how to use Pytorch and to observe the impact of regularization during training. You should test different network architectures, e.g. with hidden layers of size 128-128, 128-64-32-16, 256-128-64-32-16, 512-256-128-64-32-16, 800-800, and different activation functions (tanh, relu, sigmoid).</p>\n",
    "<p>Remember that Pytorch expects data in a different format than in the previous lab exercise: the first dimension is always the batch dimension.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs that we will use\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# To load the data we will use the script of Gaetan Marceau Caron\n",
    "# You can download it from the course webiste and move it to the same directory that contains this ipynb file\n",
    "import dataset_loader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mnist dataset \n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    !wget http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "\n",
    "# if you have it somewhere else, you can comment the lines above\n",
    "# and overwrite the path below\n",
    "mnist_path = \"./mnist.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 3 splits\n",
    "train_data, dev_data, test_data = dataset_loader.load_mnist(mnist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25927195d68>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 900\n",
    "label = train_data[1][index]\n",
    "picture = train_data[0][index]\n",
    "\n",
    "print(\"label: %i\" % label)\n",
    "plt.imshow(picture.reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n",
      "torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "first= 800\n",
    "\n",
    "image = torch.from_numpy(train_data[0][first])\n",
    "print(image.shape) # flat image of dim (784,)\n",
    "# reshape the tensor so it is represented as a batch containing a single image\n",
    "# -1 means \"all remaining elements\", here it would be equivalent to image.reshape(1, 784)\n",
    "image = image.reshape(1, -1)\n",
    "print(image.shape) # flat image of dim (1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    }
   ],
   "source": [
    "# Constructing a batched input\n",
    "batch_size = 10\n",
    "first = 20\n",
    "\n",
    "# the cat() function concatenates a list of tensor along a dimension\n",
    "batch_input = torch.cat(\n",
    "    [\n",
    "        # we reshape the image tensor so it has dimension (1, 784)\n",
    "        torch.from_numpy(image).reshape(1, -1)\n",
    "        for image in train_data[0][first:first + batch_size]\n",
    "    ],\n",
    "    # we want to concatenate on the batch dimension\n",
    "    dim=0\n",
    ")\n",
    "print(batch_input.shape)  # batch of ten flat images (10, 784)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"1.2.-Layer-initialization\">1.2. Layer initialization<a class=\"anchor-link\" href=\"#1.2.-Layer-initialization\">¶</a></h3><p>By default, Pytorch will apply Kaiming initialization to linear layers. However, I recommend you to always explicitly initialize you network by hand in the constructor.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "        # initialization are always in-place operations\n",
    "        # linear.weight is a Parameter, linear.weight.data is the tensor containing the parameter values\n",
    "        torch.nn.init.xavier_uniform_(linear.weight.data)  # Xavier/Glorot init for tanh\n",
    "        torch.nn.init.kaiming_uniform_(linear.weight.data)  # Kaiming/He init for tanh\n",
    "\n",
    "    if bias:\n",
    "        torch.nn.init.zeros_(linear.bias.data)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))\n",
    "\n",
    "class CBOW_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW_classifier, self).__init__()\n",
    "        # output first layer\n",
    "        self.output = th.nn.Linear(embedding_dim,1)\n",
    "        #init weights and bias\n",
    "        th.nn.init.xavier_uniform_(self.output.weight.data)\n",
    "        th.nn.init.zeros_(self.output.bias.data)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        zi = self.emb_table(inputs) #embedding all words for each sequence\n",
    "        z = th.sum(zi ,dim=0) #sum embedding vectors for each sequence     \n",
    "        output = th.sigmoid(self.output(z))\n",
    "        \n",
    "        return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bias' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5e3e85520042>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlinear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# initialization are always in-place operations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# linear.weight is a Parameter, linear.weight.data is the tensor containing the parameter values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bias' is not defined"
     ]
    }
   ],
   "source": [
    "def training(epochs,stepsize,txt_train,label_train,txt_dev,label_dev):\n",
    "\n",
    "    model = CBOW_classifier(vocab_size=len(dic)+1,embedding_dim=4)   \n",
    "    optimizer = th.optim.SGD(params=model.parameters(), lr =stepsize, weight_decay=1e-4)\n",
    "    \n",
    "    linear = torch.nn.Linear(10, 20, bias=bias)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"1.3.-Regularization\">1.3. Regularization<a class=\"anchor-link\" href=\"#1.3.-Regularization\">¶</a></h3><p>You can try two types of regularization (they can be combined together):</p>\n",
    "<ul>\n",
    "<li>weight decay: it is a parameter of the optimizer</li>\n",
    "<li>dropout: see slides</li>\n",
    "</ul>\n",
    "<h3 id=\"1.4.-Gradient-clipping\">1.4. Gradient clipping<a class=\"anchor-link\" href=\"#1.4.-Gradient-clipping\">¶</a></h3><p>A commong trick for training neural networks is gradient clipping: if the norm of the gradient is too big, we rescale the gradient. This trick can be used to prevent exploding gradients and also to make \"too big steps\" in the wrong direction due the use of approximate gradient computation in SGD.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_loss.backward()  # compute gradient\n",
    "torch.nn.utils.clip_grad_value_(network.parameters(), 5.)  # clip gradient if its norm exceed 5\n",
    "optimizer.step()  # update parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"1.4.-Bonus: Convolutional-Neural-Network\">1.4. Bonus: Convolutional Neural Network<a class=\"anchor-link\" href=\"#1.4.-Bonus: Convolutional-Neural-Network\">¶</a></h3><p>You can try to rely on a CNN instead of a MLP to classify MNIST images (you can still have a single layer MLP on top of convolutions, after pooling!). Note that this will requires you to reshape the input images!</p>\n",
    "<p><a href=\"https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d\">https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d</a></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t = torch.rand((10, 100))  # t is batch of 10 \"flat\" pictures\n",
    "t = t.reshape(10, 10, 10)  # we reshape t so each batch contains a 10x10 picture that is not flat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Part-2:-Variational-Auto-Encoder\">Part 2: Variational Auto-Encoder<a class=\"anchor-link\" href=\"#Part-2:-Variational-Auto-Encoder\">¶</a></h2><p>To build a new Variational Auto-Encoder, you need two networks:</p>\n",
    "<ul>\n",
    "<li>An encoder that will take as input an image and compute the parameters of list of Normal distributions</li>\n",
    "<li>A decoder that will take a sample from each Normal distribution and will output an image</li>\n",
    "</ul>\n",
    "<p>For simplicity we will assume that:</p>\n",
    "<ul>\n",
    "<li>each network as a single hidden layer of size 100</li>\n",
    "<li>the latent space contains only 2 points</li>\n",
    "</ul>\n",
    "<p>To understand exactly what a VAE is, you can:</p>\n",
    "<ul>\n",
    "<li>check the slides of Michèle Sebag</li>\n",
    "<li>check this tutorial: <a href=\"https://arxiv.org/abs/1606.05908\">https://arxiv.org/abs/1606.05908</a></li>\n",
    "</ul>\n",
    "<h3 id=\"1.2.-Encoder\">1.2. Encoder<a class=\"anchor-link\" href=\"#1.2.-Encoder\">¶</a></h3><ul>\n",
    "<li>Compute an hidden representation: $z = relu(W^1 x + b^1)$</li>\n",
    "<li>Compute the means of the normal distributions: $mu = W^2 x + b^2$</li>\n",
    "<li>Compute the log variance of the normal distributions: $log\\_sigma\\_squared = W^3 x + b^3$</li>\n",
    "</ul>\n",
    "<h2 id=\"1.2.-Decoder\">1.2. Decoder<a class=\"anchor-link\" href=\"#1.2.-Decoder\">¶</a></h2><p>This a simple MLP, nothing new here!</p>\n",
    "<h2 id=\"1.3.-Training-loss\">1.3. Training loss<a class=\"anchor-link\" href=\"#1.3.-Training-loss\">¶</a></h2><p>To compute the training loss, you must compute two terms:</p>\n",
    "<ul>\n",
    "<li>a Monte-Carlo estimation of the reconstruction loss</li>\n",
    "<li>the KL divergence between the distributions computed by the encoder and the prior</li>\n",
    "</ul>\n",
    "<p>To sample values, you can use the reparameterization trick as follows:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "e = torch.normal(0, 1., mu.shape)\n",
    "z = mu + e * torch.sqrt(torch.exp(log_sigma_squared))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>For the reconstruction loss, use the Binary Cross Entropy loss:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_builder = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>The formula of the KL divergence with the prior is as follows:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "-0.5 * torch.sum(1 + log_sigma_squared - mu.pow(2) - log_sigma_squared.exp())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"1.4.-Recomended-hyper-parameters\">1.4. Recomended hyper parameters<a class=\"anchor-link\" href=\"#1.4.-Recomended-hyper-parameters\">¶</a></h2><ul>\n",
    "<li>Optimizer: Adam</li>\n",
    "<li>N. epochs: 50</li>\n",
    "<li>Use gradient clipping!</li>\n",
    "<li>Large batch size, e.g. 128</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use itertools.chain to join parameters of the two networks\n",
    "optimizer = torch.optim.Adam(itertools.chain(encoder.parameters(), decoder.parameters()))\n",
    "torch.nn.utils.clip_grad_value_(itertools.chain(encoder.parameters(), decoder.parameters()), 5.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"1.5.-Generate-new-images\">1.5. Generate new images<a class=\"anchor-link\" href=\"#1.5.-Generate-new-images\">¶</a></h2><p>Note: they will be blurry, but that's ok!</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "e = torch.normal(0, 1., (10, 2))\n",
    "images = decoder(e).sigmoid()\n",
    "\n",
    "for i in range(10):\n",
    "    picture = images[i].clone().detach().numpy()\n",
    "    plt.imshow(picture.reshape(28,28), cmap='Greys')\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
