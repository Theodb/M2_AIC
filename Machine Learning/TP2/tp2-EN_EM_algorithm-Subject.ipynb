{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EM Algorithm** (Expectation - Maximization)\n",
    "\n",
    "We're going to implement the EM algorithm for a mixture of Bernoullis\n",
    "\n",
    "The Expectation-Maximization algo. is used in sk-learn, for instance in GMMs: http://scikit-learn.org/stable/modules/mixture.html#estimation-algorithm-expectation-maximization\n",
    "\n",
    "Additional notes are available here: https://allauzen.github.io/articles/MixturesAndEM/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Think for a bit\n",
    "\n",
    "Is EM a supervised or unsupervised learning algorithm ? \n",
    "\n",
    "What kind of data set is MNIST ? \n",
    "\n",
    "Then, what are we going to do ?  How is this called ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implement the EM algo. for a mixture of Bernoulli (laws)\n",
    "- The cluster number K should be an argument of the function\n",
    "- A maximum number of iterations, *MaxIt*, should act as stopping condition\n",
    "- During the E step, compute and *store* the log-likelihood of the data, so as to monitor its evolution along iterations (~epochs)\n",
    "\n",
    "Apply the algorithm on MNIST:\n",
    "- try out K=5,10,15\n",
    "- Visualize the images coresponding to each cluster's paraemeters. Would that be as straightforward in a Gaussian model (visualizing all the model's parameters?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we load the whole data set at once, so as to manipulate it directly (using numpy arrays)\n",
    "## for very large data sets, one needs to read on-the-fly (at least at production time, not at the debugging stage)\n",
    "\n",
    "# ## Load the dataset -- (python2)\n",
    "# import cPickle, gzip\n",
    "# with gzip.open('./tp1-mnist.pkl.gz','rb') as f :\n",
    "#     train_set, valid_set, test_set = cPickle.load(f)\n",
    "\n",
    "### Load the dataset -- (python3)\n",
    "import pickle, gzip\n",
    "with gzip.open('./mnist.pkl.gz','rb') as f :\n",
    "    u = pickle._Unpickler( f )\n",
    "    u.encoding = 'latin1'\n",
    "    train_set, valid_set, test_set = u.load()\n",
    "\n",
    "## split train an dtest data, to avoid inadvertently cheating.\n",
    "unlabelled_dataset = train_set[0].copy()\n",
    "labels_for_final_accuracy_measurement = train_set[1].copy()\n",
    "del train_set\n",
    "\n",
    "## these would be useful, for now we don't use them\n",
    "del valid_set  \n",
    "del test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## binarize so as to go into Bernoulli-space\n",
    "threshold = 0.3\n",
    "for i in range(unlabelled_dataset.shape[0]): \n",
    "    unlabelled_dataset[i,:]= 1.0*(unlabelled_dataset[i]>threshold)\n",
    "## (unlabelled_dataset[i]>threshold) is now an array of Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## inspect the new values \"by hand\", to check.\n",
    "unlabelled_dataset[0][300:400] ## reading \n",
    "## we do NOT convert to Integers, instead we stay in floats. Because we'll compute averages later (thus, divide..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for debugging, we may want to use a small piece of the sample\n",
    "subSampling = 100  ## data size will be divided by a factor \"subSampling\"\n",
    "## in the end, we may use all of it (set this to 1) \n",
    "\n",
    "## sort data to make sure all classes are equally present ##\n",
    "ordre = np.argsort(labels_for_final_accuracy_measurement)\n",
    "unlabelled_dataset = unlabelled_dataset[ordre]\n",
    "labels_for_final_accuracy_measurement = labels_for_final_accuracy_measurement[ordre]\n",
    "\n",
    "## do the sub-sampling\n",
    "dataset = unlabelled_dataset[::subSampling].copy()\n",
    "subSampledLabels = labels_for_final_accuracy_measurement[::subSampling].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "       9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check it:\n",
    "subSampledLabels[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## when you code, start by writing on a copy of the data \n",
    "## (not the data themselves: otherwise on the second run of the jupyter cell, you'll have run twice)\n",
    "K=10\n",
    "MaxIt=4\n",
    "dim = (dataset.shape)[1]\n",
    "k=4\n",
    "xi=dataset[0]\n",
    "theta = np.zeros((K, dim))\n",
    "\n",
    "## compute the proba of an image xi, \n",
    "## given its cluster k and parameters theta \n",
    "def compute_P_Xi_given_k_and_theta(xi, k, theta):\n",
    "    #subSLabels=subSampledLabels.copy()\n",
    "    logpi=xi*np.log(theta+10**-8)+(1-xi)*np.log(1-theta+10**-8)\n",
    "    output=sum(logpi)\n",
    "    return output\n",
    "\n",
    "outputForDebug = compute_P_Xi_given_k_and_theta(xi, k, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08 -1.84206807e+02 -1.84206807e+02\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08 -1.84206807e+02 -1.84206807e+02\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08 -1.84206807e+02\n",
      " -1.84206807e+02 -1.84206807e+02 -1.84206807e+02 -1.84206807e+02\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08\n",
      "  9.99999989e-08  9.99999989e-08  9.99999989e-08  9.99999989e-08]\n"
     ]
    }
   ],
   "source": [
    "print(outputForDebug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 784 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] [[0.07495811 0.14767547 0.12806177 ... 0.02121543 0.1792508  0.17086294]\n",
      " [0.13972571 0.08327242 0.05415162 ... 0.03734071 0.17480676 0.07727137]\n",
      " [0.03705134 0.08417405 0.1527212  ... 0.12649736 0.06986583 0.14957424]\n",
      " ...\n",
      " [0.05734223 0.12467498 0.17446599 ... 0.10768716 0.09748848 0.02795721]\n",
      " [0.11447567 0.12510867 0.13298956 ... 0.03634823 0.04807512 0.0726098 ]\n",
      " [0.11994    0.07979479 0.05041766 ... 0.08947527 0.15006547 0.10348553]] 10\n"
     ]
    }
   ],
   "source": [
    "print(Nex,dim, classFrequencies, theta, affectations, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-37242be359c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#    affectations[:,i] / np.sum(affectations[:,i])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0maffectations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "affectations = np.random.random((K, Nex))\n",
    "#for i in range(Nex):\n",
    "#    affectations[:,i] / np.sum(affectations[:,i])\n",
    "affectations.shape\n",
    "data=dataset.shape(10, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14767547 0.08327242 0.08417405 0.07443183 0.03836348 0.11875492\n",
      " 0.12374938 0.12467498 0.12510867 0.07979479]\n"
     ]
    }
   ],
   "source": [
    "print(affectations[:,1] / np.sum(affectations[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,500) (500,784) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-34c89efebf76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtheta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maffectations\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,500) (500,784) "
     ]
    }
   ],
   "source": [
    "data=dataset\n",
    "theta=affectations*dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-b449aab01679>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m## step M: update of \"pi_k, mu_k\" (classFrequencies, theta)  ##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mclassFrequencies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maffectations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maffectations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "## python-note: at debug time, don't write a function, instead, write directly in the main\n",
    "## when evreything works well, encapsulate the piece of code into a function for later re-use (and lisibility)\n",
    "#def AlgoEM(dataset, K, MaxIt, labels_for_final_accuracy_measurement):\n",
    "    \n",
    "\n",
    "## we set the seed to a constant so as to have repeatable experiences \n",
    "## (at debug time, and when comparing hyper-parameters)\n",
    "np.random.seed(42)\n",
    "\n",
    "## we'll assume dataset[i,j] is the value of pixel j of image i.\n",
    "Nex = (dataset.shape)[0] ## number of examples (= number of images)\n",
    "dim = (dataset.shape)[1] ## dimension of parameters space (=number of pixels per image, in the Bernoulli case)\n",
    "classFrequencies = np.zeros(K) ## denoted pi_k \n",
    "theta = np.zeros((K, dim))   ## denoted theta_{k,j}\n",
    "\n",
    "## initialization ##\n",
    "affectations = np.random.random((K, Nex)) ## denoted a_{k,i}\n",
    "## normalization step (needed to make the )\n",
    "for i in range(Nex):\n",
    "    affectations[:,i] / np.sum(affectations[:,i])\n",
    "    \n",
    "## main loop ##\n",
    "for iteration in range(0,MaxIt,1):\n",
    "    #if iteration%10 == 0:\n",
    "    #    print(\"iteration numero\"+str(iteration))\n",
    "       \n",
    "\n",
    "    ######################\n",
    "    ## step M: update of \"pi_k, mu_k\" (classFrequencies, theta)  ##\n",
    "    for k in range(K):\n",
    "        classFrequencies[k]=np.sum(affectations[k])/sum(affectations)\n",
    "        \n",
    "    \n",
    "    theta=np.dot(affectations*dataset)\n",
    "    #For     \n",
    "    #    theta[k]=np.dot(sum(affectation[k])*\n",
    "    \n",
    "\n",
    "    ######################\n",
    "    ## step E: update of \"a_ik\" (affectations) ##\n",
    "    ## TODO \n",
    "\n",
    "    ## monitoring of the quality of te clustering ##\n",
    "    #LogLikelihood = np.log(np.max(affectations, axis=0)) \n",
    "    #print(np.mean(LogLikelihood))\n",
    "    ## TODO: record the monitoring into an array, to be able to later plot it\n",
    "\n",
    "return classFrequencies[k]\n",
    "##########################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classFrequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LogVraisemblance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can visualize the parameters theta, cluster by cluster\n",
    "for k in range(K):\n",
    "    plt.imshow(theta[k].reshape(28,28) , matplotlib.pyplot.cm.jet)\n",
    "    matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: after checking your code works, put it inside a function and call that function \n",
    " (instead of running code in the *main()* directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affectations, theta, classFrequencies = AlgoEM(dataset, K, MaxIt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's monitor something more interpretable than the log-likelihood... what could it be ? \n",
    "\n",
    "Trick: you may use the function np.argmax(), and maybe just a couple of labels from a couple of examples\n",
    "\n",
    "**Quesiton**: would this strategy be possible for a purely unsupervised task ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: (later) \n",
    "## what , 0,1,2,...9\n",
    "# prediction = np.argmax(affectations, axis=0)\n",
    "# score = np.mean(prediction == labels_for_final_accuracy_measurement[:subSampling])\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add this interesting metric into your function, and add some plot of it from the function as well (possibly along wth the log-likelihood plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you may play with the hyper-parameters.\n",
    "\n",
    "What is the best value of the main hyper-parameter ? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
