{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Models for Image Classification\n",
    "\n",
    "\n",
    "<img src=\"./images/transfer_learning.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "- PyTorch has many pretrained models. \n",
    "- All of these models have been trained on Imagenet which consists of millions of images across 1000 categories. \n",
    "- What we want to do with pretrained models is freeze the early layers, and replace the classification module with our own. \n",
    "\n",
    "### Process\n",
    "\n",
    "1. Load in pre-trained weights from a network trained on a large dataset\n",
    "2. Freeze all the weights in the lower (convolutional) layers\n",
    "    * Layers to freeze can be adjusted depending on similarity of task to large training dataset\n",
    "3. Replace the classifier (fully connected) part of the network with a custom classifier\n",
    "    * Number of outputs must be set equal to the number of classes\n",
    "4. Train only the custom classifier (fully connected) layers for the task\n",
    "    * Optimizer model classifier for smaller dataset\n",
    "    \n",
    "**The idea behind pre-training is the early convolutional layers of a cnn extract features that are relevant for many image recognition tasks.** \n",
    "\n",
    "**The later, fully-connected layers, specialize to the specific dataset by learning higher-level features.**\n",
    "\n",
    "Therefore, we can use the already trained convolutional layers while training only the fully-connected layers on our own dataset. \n",
    "\n",
    "Pre-trained networks have proven to be reasonably successful for a variety of tasks, and result in a significant reduction in training time and usually increases in performance. \n",
    "\n",
    "\n",
    "https://pytorch.org/docs/stable/torchvision/models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import seaborn as sns\n",
    "# PyTorch\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch\n",
    "from torch import optim, cuda\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Data science tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Image manipulations\n",
    "from PIL import Image\n",
    "# Useful for examining network\n",
    "#from torchsummary import summary\n",
    "# Timing utility\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# Printing out all outputs\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on gpu: False\n"
     ]
    }
   ],
   "source": [
    "# Location of data\n",
    "datadir = '../data'\n",
    "traindir = 'dataset'\n",
    "testdir = 'dataset-test'\n",
    "\n",
    "save_file_name = 'vgg16-transfer-4.pt'\n",
    "checkpoint_path = 'vgg16-transfer-4.pth'\n",
    "\n",
    "# Change to fit hardware\n",
    "# batch_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "# Whether to train on a gpu\n",
    "train_on_gpu = cuda.is_available()\n",
    "print(f'Train on gpu: {train_on_gpu}')\n",
    "\n",
    "# Number of gpus\n",
    "if train_on_gpu:\n",
    "    gpu_count = cuda.device_count()\n",
    "    print(f'{gpu_count} gpus detected.')\n",
    "    if gpu_count > 1:\n",
    "        multi_gpu = True\n",
    "    else:\n",
    "        multi_gpu = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torchvision.datasets.folder import ImageFolder, default_loader\n",
    "from torchvision.datasets.utils import download_url, check_integrity\n",
    "\n",
    "class EdibleWildPlantsDataset(ImageFolder):\n",
    "    \"\"\"\n",
    "    The 'Edible Wild Plants' dataset from kaggle.\n",
    "\n",
    "    https://www.kaggle.com/gverzea/edible-wild-plants\n",
    "\n",
    "    Args:\n",
    "        root: the location where to store the dataset\n",
    "        suffix: path to the train/valid/sample dataset. See folder structure.\n",
    "        transform (callable, optional): A function/transform that takes in\n",
    "            an PIL image and returns a transformed version.\n",
    "            E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that\n",
    "            takes in the target and transforms it.\n",
    "        loader: A function to load an image given its path.\n",
    "        download: if ``True``, download the data.\n",
    "\n",
    "\n",
    "    The folder structure of the dataset is as follows::\n",
    "\n",
    "...\n",
    "\n",
    "    \"\"\"\n",
    "    url = \"https://www.kaggle.com/gverzea/edible-wild-plants/downloads/edible-wild-plants.zip/5\"\n",
    "    filename = \"edible-wild-plants.zip\"\n",
    "    checksum = \"22934d7bb30c6e1ebbb805fd186afd6f\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 suffix: str,\n",
    "                 transform=None,\n",
    "                 target_transform=None,\n",
    "                 loader=default_loader,\n",
    "                 download=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "\n",
    "        if download:\n",
    "            self._download()\n",
    "            self._extract()\n",
    "\n",
    "        #if not self._check_integrity():\n",
    "        #    raise RuntimeError(\"Dataset not found or corrupted. \"\n",
    "        #                       \"You can use download=True to download it\")\n",
    "\n",
    "        path = os.path.join(self.root, suffix)\n",
    "        print(f\"Loading data from {path}.\")\n",
    "        assert os.path.isdir(path), f\"'{suffix}' is not valid.\"\n",
    "\n",
    "        super().__init__(path, transform, target_transform, loader)\n",
    "    \n",
    "    #def __getitem__(self, idx):\n",
    "        #item = super().__getitem__(idx)\n",
    "        #return (item[0], np.ndarray(item[1]))\n",
    "\n",
    "    def _download(self):\n",
    "        if self._check_integrity():\n",
    "            print(\"Dataset already downloaded and verified.\")\n",
    "            return\n",
    "\n",
    "        root = self.root\n",
    "        print(\"Downloading dataset... (this might take a while)\")\n",
    "        download_url(self.url, root, self.filename, self.checksum)\n",
    "\n",
    "    def _extract(self):\n",
    "        import zipfile\n",
    "        path_to_zip = os.path.join(self.root, self.filename)\n",
    "        with zipfile.ZipFile(path_to_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.root)\n",
    "        with zipfile.ZipFile(os.path.join(self.root, 'datasets.zip'), 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.root)\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        \n",
    "        path_to_zip = os.path.join(self.root, self.filename)\n",
    "        print (path_to_zip)\n",
    "        return check_integrity(path_to_zip, self.checksum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "image_transforms = {\n",
    "    # Train uses data augmentation\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])  # Imagenet standards\n",
    "    ]),\n",
    "    # Test does not use augmentation\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/dataset.\n",
      "Loading data from ../data/dataset-test.\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'train' : EdibleWildPlantsDataset(datadir, traindir, transform=image_transforms['train'], download=False),\n",
    "    'test' : EdibleWildPlantsDataset(datadir, testdir, transform=image_transforms['test'], download=False)\n",
    "}\n",
    "\n",
    "# Dataloader iterators\n",
    "data_loaders = {\n",
    "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "    'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(data_loaders['train'])\n",
    "features, labels = next(train_iter)\n",
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = len(data['train'].classes)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "#plt.rcParams['figure.figsize'] = (8.0, 8.0) # set default size of plots\n",
    "# functions to show an image\n",
    "\n",
    "def imshow(img):\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    #print (npimg.shape)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(data_loader['train'])\n",
    "images, labels = dataiter.next()\n",
    "#print (images.shape)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print('   -    '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the model with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Early layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Freeze early layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add on Custom Classifier\n",
    "\n",
    "We'll train a classifier consisting of the following layers\n",
    "\n",
    "* Fully connected with ReLU activation (n_inputs, 256)\n",
    "* Dropout with 40% chance of dropping\n",
    "* Fully connected with log softmax output (256, n_classes)\n",
    "\n",
    "To build our custom classifier, we use the `nn.Sequential()` module which allows us to specify each layer one after the other. We assign our custom classifier to the final `classifier` layer in the already trained vgg network. When we add on the extra layers, they are set to `require_grad=True` by default. These will be the only layers that are trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Dropout(p=0.5)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace)\n",
       "  (5): Dropout(p=0.5)\n",
       "  (6): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.4)\n",
       "    (3): Linear(in_features=256, out_features=62, bias=True)\n",
       "    (4): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = model.classifier[6].in_features\n",
    "\n",
    "# Add on classifier\n",
    "model.classifier[6] = nn.Sequential(\n",
    "    nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.4),\n",
    "    nn.Linear(256, n_classes), nn.LogSoftmax(dim=1))\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output will be log probabilities which we can then use in the Negative Log Likelihood Loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135,325,310 total parameters.\n",
      "1,064,766 training parameters.\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_on_gpu:\n",
    "    model = model.to('cuda')\n",
    "\n",
    "    if multi_gpu:\n",
    "        model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Alfalfa'),\n",
       " (1, 'Asparagus'),\n",
       " (2, 'Blue Vervain'),\n",
       " (3, 'Broadleaf Plantain'),\n",
       " (4, 'Bull Thistle'),\n",
       " (5, 'Cattail'),\n",
       " (6, 'Chickweed'),\n",
       " (7, 'Chicory'),\n",
       " (8, 'Cleavers'),\n",
       " (9, 'Coltsfoot')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.class_to_idx = data['train'].class_to_idx\n",
    "model.idx_to_class = {\n",
    "    idx: class_\n",
    "    for class_, idx in model.class_to_idx.items()\n",
    "}\n",
    "\n",
    "list(model.idx_to_class.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 4096])\n",
      "torch.Size([256])\n",
      "torch.Size([62, 256])\n",
      "torch.Size([62])\n"
     ]
    }
   ],
   "source": [
    "for p in optimizer.param_groups[0]['params']:\n",
    "    if p.requires_grad:\n",
    "        print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model, optimizer = load_checkpoint(path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Load in Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          train_loader,\n",
    "          save_file_name,\n",
    "          max_epochs_stop=3,\n",
    "          n_epochs=10,\n",
    "          print_every=2):\n",
    "    \"\"\"Train a PyTorch Model\n",
    "\n",
    "    Params\n",
    "    --------\n",
    "        model (PyTorch model): cnn to train\n",
    "        criterion (PyTorch loss): objective to minimize\n",
    "        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n",
    "        train_loader (PyTorch dataloader): training dataloader to iterate through\n",
    "        save_file_name (str ending in '.pt'): file path to save the model state dict\n",
    "        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n",
    "        n_epochs (int): maximum number of training epochs\n",
    "        print_every (int): frequency of epochs to print training stats\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        model (PyTorch model): trained cnn with best weights\n",
    "        history (DataFrame): history of train and validation loss and accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Early stopping intialization\n",
    "    epochs_no_improve = 0\n",
    "    history = []\n",
    "\n",
    "    # Number of epochs already trained (if using loaded in model weights)\n",
    "    try:\n",
    "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
    "    except:\n",
    "        model.epochs = 0\n",
    "        print(f'Starting Training from Scratch.\\n')\n",
    "\n",
    "    overall_start = timer()\n",
    "\n",
    "    # Main loop\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # keep track of training loss each epoch\n",
    "        train_loss = 0.0\n",
    "\n",
    "        train_acc = 0\n",
    "\n",
    "        # Set to training\n",
    "        model.train()\n",
    "        start = timer()\n",
    "\n",
    "        # Training loop\n",
    "        for ii, (data, target) in enumerate(train_loader):\n",
    "            # Tensors to gpu\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Predicted outputs are log probabilities\n",
    "            output = model(data)\n",
    "\n",
    "            # Loss and backpropagation of gradients\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track train loss by multiplying average loss by number of examples in batch\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "            # Calculate accuracy by finding max log probability\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            # Need to convert correct tensor from int to float to average\n",
    "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "            # Multiply average accuracy times the number of examples in batch\n",
    "            train_acc += accuracy.item() * data.size(0)\n",
    "\n",
    "            # Track training progress\n",
    "            print(\n",
    "                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
    "                end='\\r')\n",
    "            \n",
    "\n",
    "            # Don't need to keep track of gradients\n",
    "            with torch.no_grad():\n",
    "                # Set to evaluation mode\n",
    "                model.eval()\n",
    "\n",
    "                # Calculate average losses\n",
    "                train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "                # Calculate average accuracy\n",
    "                train_acc = train_acc / len(train_loader.dataset)\n",
    "\n",
    "                history.append([train_loss, train_acc])\n",
    "\n",
    "    # Attach the optimizer\n",
    "    model.optimizer = optimizer\n",
    "    # Record overall time and print out stats\n",
    "    total_time = timer() - overall_start\n",
    "    \n",
    "    # Format history\n",
    "    history = pd.DataFrame(\n",
    "        history,\n",
    "        columns=['train_loss', 'train_acc'])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been trained for: 0 epochs.\n",
      "\n",
      "Epoch: 0\t19.02% complete. 305.32 seconds elapsed in epoch.\r"
     ]
    }
   ],
   "source": [
    "model, history = train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    data_loaders['train'],\n",
    "    save_file_name=save_file_name,\n",
    "    max_epochs_stop=5,\n",
    "    n_epochs=1,\n",
    "    print_every=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for c in ['train_loss']:\n",
    "    plt.plot(\n",
    "        history[c], label=c)\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Negative Log Likelihood')\n",
    "plt.title('Training Losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for c in ['train_acc']:\n",
    "    plt.plot(\n",
    "        100 * history[c], label=c)\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.title('Training Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataiter = iter(data_loaders['test'])\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print('GroundTruth: ',' -  '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save (and reload) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model, path):\n",
    "    \"\"\"Save a PyTorch model checkpoint\n",
    "\n",
    "    Params\n",
    "    --------\n",
    "        model (PyTorch model): model to save\n",
    "        path (str): location to save model. Must start with `model_name-` and end in '.pth'\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        None, save the `model` to `path`\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = path.split('-')[0]\n",
    "    assert (model_name in ['vgg16', 'resnet50'\n",
    "                           ]), \"Path must have the correct model name\"\n",
    "\n",
    "    # Basic details\n",
    "    checkpoint = {\n",
    "        'class_to_idx': model.class_to_idx,\n",
    "        'idx_to_class': model.idx_to_class,\n",
    "        'epochs': model.epochs,\n",
    "    }\n",
    "\n",
    "    # Extract the final classifier and the state dictionary\n",
    "    if model_name == 'vgg16':\n",
    "        # Check to see if model was parallelized\n",
    "        if multi_gpu:\n",
    "            checkpoint['classifier'] = model.module.classifier\n",
    "            checkpoint['state_dict'] = model.module.state_dict()\n",
    "        else:\n",
    "            checkpoint['classifier'] = model.classifier\n",
    "            checkpoint['state_dict'] = model.state_dict()\n",
    "\n",
    "    elif model_name == 'resnet50':\n",
    "        if multi_gpu:\n",
    "            checkpoint['fc'] = model.module.fc\n",
    "            checkpoint['state_dict'] = model.module.state_dict()\n",
    "        else:\n",
    "            checkpoint['fc'] = model.fc\n",
    "            checkpoint['state_dict'] = model.state_dict()\n",
    "\n",
    "    # Add the optimizer\n",
    "    checkpoint['optimizer'] = model.optimizer\n",
    "    checkpoint['optimizer_state_dict'] = model.optimizer.state_dict()\n",
    "\n",
    "    # Save the data to the path\n",
    "    torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_checkpoint(model, path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(path):\n",
    "    \"\"\"Load a PyTorch model checkpoint\n",
    "\n",
    "    Params\n",
    "    --------\n",
    "        path (str): saved model checkpoint. Must start with `model_name-` and end in '.pth'\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        None, save the `model` to `path`\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the model name\n",
    "    model_name = path.split('-')[0]\n",
    "    assert (model_name in ['vgg16', 'resnet50'\n",
    "                           ]), \"Path must have the correct model name\"\n",
    "\n",
    "    # Load in checkpoint\n",
    "    checkpoint = torch.load(path)\n",
    "\n",
    "    if model_name == 'vgg16':\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        # Make sure to set parameters as not trainable\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.classifier = checkpoint['classifier']\n",
    "\n",
    "    elif model_name == 'resnet50':\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        # Make sure to set parameters as not trainable\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.fc = checkpoint['fc']\n",
    "\n",
    "    # Load in the state dict\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'{total_params:,} total parameters.')\n",
    "    total_trainable_params = sum(\n",
    "        p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'{total_trainable_params:,} total gradient parameters.')\n",
    "\n",
    "    # Move to gpu\n",
    "    if multi_gpu:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    if train_on_gpu:\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    # Model basics\n",
    "    model.class_to_idx = checkpoint['class_to_idx']\n",
    "    model.idx_to_class = checkpoint['idx_to_class']\n",
    "    model.epochs = checkpoint['epochs']\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = checkpoint['optimizer']\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    return model, optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
