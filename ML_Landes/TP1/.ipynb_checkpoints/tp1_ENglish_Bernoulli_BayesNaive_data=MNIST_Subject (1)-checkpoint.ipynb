{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook:\n",
    "check a few things: how to save (Ctrl+S), how to run a code block (Caps+Enter),how to move (arrows), how to create a cell (find it yourself ! : browse the menus a bit before starting !).\n",
    "Note: in jupyter, like in ipython, if you just write a variable name at the end of your code (or running cell), its value will be displayed (you do not need to write \"print(myVariable)\", it's enough to write \"myVariable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'some string with some value, typed into the last line of the cell'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_version()\n",
    "print(python_version()) #does the same as above\n",
    "\"some string with some value, typed into the last line of the cell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.7.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(python_version()) #does the same as above\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### this is useful only if you unfortunately run suing python2. If you run python3, these lines are effect-less.\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Goal of the tutorial:\n",
    "\n",
    "The aim of this tutorial is to implement the Naive Bayesian calssifier on the MNIST image data set. (optionally, also on the 20newsgroups text data set).\n",
    "\n",
    "Let's begin with MNIST. It's in the GDrive, and may be accessed \"locally\" at: \n",
    "https://www.lri.fr/~flandes/share/ \n",
    "    \n",
    "MNIST contains images (observations, inputs) of single hand-written digits and the corresponding classes (labels, Ground Truth). \n",
    "So it is a 10-classes classification problem (0 to 9).\n",
    "\n",
    "\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cPickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-fa0c26bb7873>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m## Load the dataset -- (python2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./mnist.pkl.gz'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cPickle'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "## Load the dataset -- (python2)\n",
    "import cPickle, gzip\n",
    "with gzip.open('./mnist.pkl.gz','rb') as f :\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "\n",
    "# Load the dataset -- (python3)\n",
    "# import pickle, gzip\n",
    "# with gzip.open('./tp1-mnist.pkl.gz','rb') as f :\n",
    "#     u = pickle._Unpickler( f )\n",
    "#     u.encoding = 'latin1'\n",
    "#     train_set, valid_set, test_set = u.load()\n",
    "\n",
    "NLABELS=10\n",
    "print(str(len(train_set[0]))+\" training examples\")\n",
    "# exemple: \n",
    "im    = train_set[0][0] # the first image\n",
    "label = train_set[1][0] # its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im.reshape(28,28) , matplotlib.pyplot.cm.gray)    \n",
    "# plt.contourf(im.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content of data\n",
    "\n",
    "Images are stored as numpy arrays, where each line is an image, i.e. a 784-rows vector (28x28=784). Each pixel encodes a Grey value between 0 and 1.\n",
    "* What do train_set, valid_set, test_set contain ? (size, type, ... )\n",
    "* Write a function binarize_image(image, threshold) with a default threshold=0.5 to project an image onto binary pixels (black&white intead of grey levels)\n",
    "* Try various thresholds and look for differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_image(image, threshold=0.5):\n",
    "    binarized_image = image.copy()\n",
    "    ## TODO \n",
    "    return binarized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to know the data\n",
    "Here are a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting images and their labels\n",
    "images = train_set[0]\n",
    "labels = train_set[1]\n",
    "images_test = test_set[0]\n",
    "labels_test = test_set[1]\n",
    "print(\"shape d'images  :\"+str(images.shape))\n",
    "print(\"shape de labels :\"+str(labels.shape))\n",
    "# values\n",
    "print(\"val. max  \", images.max())\n",
    "print(\"val. min  \", images.min())\n",
    "print(\"val. mean \", images.mean())\n",
    "print(\"val. var  \", images.var())\n",
    "\n",
    "\n",
    "# how many images represent the digit \"5\" ?\n",
    "print(\"Nombre d'images dont le label est 5 :\"+str((labels==5).sum()))\n",
    "# labels==5 returns an ndarray of same dimensionality as labels but filled with booleans.\n",
    "# if the label is 5, True (1), else, False (0).\n",
    "# On peut se servir de cet ndarray pour faire du slicing de images\n",
    "fives = (images[labels==5])\n",
    "print(\"fives contains all images of which the label is 5, there number is:\"+str(fives.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian (model chosen: Bernoulli)\n",
    "Observations (images) are initially continuous. We need to project these data onto the space accessible to our model. In the case of Bernoulli, each pixel can only be 0 or 1 (intermediate values cannot be sampled from a Bernoulli law).\n",
    "\n",
    "### pre-processing (of the data -what else?)\n",
    "\n",
    "- Apply the thresholding operation to all *train* and *test* data. (it may take ~O(1) minute: try your function on test images, they are fewer)\n",
    "- Check it on a couple of images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Naive Bayesian, Bernoulli\n",
    "\n",
    "To implement this classifier, there are only two steps:\n",
    "- estimating parameters $\\theta$ (including the prior probability for each class).\n",
    "- inference itself:\n",
    "    - knowing the parameters, return the posterior probability of classes\n",
    "    - (for each image) infer the class (given the model, and this image' data)\n",
    "\n",
    "\n",
    "To-do list:\n",
    "Before coding, **pen and paper** !\n",
    "- Which are the parameters $\\theta$? Define them precisely.\n",
    "- Write down the exact equations for each step; ideally, down to the pixel level.\n",
    "\n",
    "If you don't know how to start, start with the end: write down the probability P(y|X) (see your lecture notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that **you know exactly what you want**, write down the functions needed.\n",
    "\n",
    "Check them at several stages: \n",
    "- represent the parameters as images or graphs, (choose adequately)\n",
    "- check your prediction on a couple of examples (not just 1 !!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract the parameters (those associated to each class, which are essentially \"representatives\")\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the priors (for each class)\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a function that computes the posterior, for a given image\n",
    "# trick: when computing log(p), one should type log(1+EPSILON), to avoid float overflow errors (when p~0)\n",
    "def computePosteriors(image, EPSILON=1e-5):\n",
    "    posteriors = np.zeros([NLABELS,1])\n",
    "    ## TODO\n",
    "    return posteriors\n",
    "\n",
    "## TODO: debug your function on a single image, of course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [option 1: apply your function to all the train/test data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePrediction(images):\n",
    "    N = images.shape[0]\n",
    "    posteriorsAllImages = np.zeros(N)\n",
    "    prediction_train = np.zeros(N, dtype=int)\n",
    "    for i in range(N):\n",
    "        posteriorsAllImages[i] = computePosteriors(images_test[i], EPSILON=1e-5)\n",
    "        prediction[i] = ## TODO: one-liner. See the help page of: np.argmax()\n",
    "    return prediction\n",
    "\n",
    "#prediction_train = makePrediction(images)\n",
    "prediction_test = makePrediction(images_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [option 2: re-write your function in a more pythonic way, handling all images at once]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##[option 2]: much more pythonic: array-wise computation \n",
    "##(much much faster)\n",
    "def computePosteriorsAllImages(images, EPSILON=1e-5):\n",
    "    N = images.shape[0]\n",
    "    posteriorsAllImages = np.zeros([N, NLABELS])\n",
    "    ## TODO : re-write the inside of computePosteriors() here, accounting for the new shape of \"images\"\n",
    "    return posteriorsAllImages\n",
    "posteriorsAllImages = computePosteriors(images)\n",
    "prediction_test ## = TODO: one-liner, same as above, but again, array shapes are slightly different\n",
    "#prediction_train ## = TODO (same as above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: \n",
    "\n",
    "at this point, you should have two arrays, *prediction_test_data* and *prediction_train*\n",
    "\n",
    "## evaluation of the quality of the classifier\n",
    "evaluate the classifier on the test data:\n",
    "- compute the error rate\n",
    "- show the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion = np.zeros((10,10))\n",
    "## showing the confusion matrix with a color map (\"heat map\")\n",
    "import pylab as pl\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion)\n",
    "pl.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "pl.xlabel('True')\n",
    "pl.ylabel('Pred.')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## checking using SciKit-learn\n",
    "\n",
    "We're going to check results by cmparing with SKlearn's implementation' output.\n",
    "- reproduce the experiment, using \n",
    "- try also with a Gaussian model (still Naive Bayes, but Gaussian instead of Bernoulli, still in SKlearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: choose which class you want to use from the package sklearn.naive_bayes\n",
    "from sklearn.naive_bayes import ## TODO \n",
    "## note: for a Gaussian model, use the data BEFORE binarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sparse data : 20newsgroups\n",
    "\n",
    "Let's hop to text classification. Texts are characterized by very sparse distributions (of words).\n",
    "\n",
    "We'll use the dataset 20newsgroups, which is available within SKlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories=None\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,)\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vectorizer = HashingVectorizer(stop_words='english',binary=True)\n",
    "X_train = vectorizer.transform(data_train.data)\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "\n",
    "# test set\n",
    "y_train, y_test = data_train.target, data_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe the data (size, type..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train and test the Bernoulli Bayesian classifier using SKlearn's default (hyper-parameters) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hint: you will need to use something like this:\n",
    "#     clf = YourChosenMethod(parameters=ChosenValueOfParameters)\n",
    "#     clf.fit(X_train,y_train)\n",
    "#     clf.score(X_test,y_test)\n",
    "\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are these default parameters ? What do they mean and what values were chosen / What choices were chosen ?\n",
    "- How to get a classifier without smoothing ? Do you expect it to work better ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of smoothing\n",
    "\n",
    "- Try out several smoothing constants, from $1$ to $10^{-15}$ by steps of factors of $10$\n",
    "- compute and report the train and test error rates for each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the curves for these error rates as a function of the smoothing constant(code given below)\n",
    "- What do you see ? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.plot(alphas, Test_score, 'x-')\n",
    "ax.plot(alphas, Train_score, 'o-')\n",
    "pl.title('Training and Testing score for different values of smoothing')\n",
    "pl.xlabel('Smoothing')\n",
    "pl.ylabel('Score')\n",
    "pl.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations ! You have performed your first hyper-parameter optimization ! \n",
    "\n",
    "Now the need for a third independent set (a.k.a. the vlaidation set) should be very clear to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial classifier\n",
    "\n",
    "see https://en.wikipedia.org/wiki/Multinomial_distribution for catching-up on maths.\n",
    "\n",
    "same questions: (Copy-Paste is your friend, whis question may take you less than 5 min)\n",
    "- train and test the SKlearn model with default parameters\n",
    "- What are these default parameters ? What do they mean and what values were chosen / What choices were chosen ?\n",
    "- What is the impact of smoothing ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crucial point: did we \"cheat\" ?\n",
    "- Visualize a couple more data examples, in particular take them all from the same class. Isn't there some piece  of information that we use here, and which looks a bit like \"cheating\" ?\n",
    "- Now import data, again using *sklearn.datasets.fetch_20newsgroups()*, but with other values for the argument \"remove\". How do your classifiers (Bernoulli / Multionmial) change ? \n",
    "- Why ? What do you think of this ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Think a bit about what you did\n",
    "\n",
    "What did we do here ? How much did we build an AI ? In the image case, independence of the pixels had a very clear interpretation.  Here as well, the \"Naive\" assumption of independence has a very clear interpretation. What do you think of this assumption ? \n",
    "\n",
    "Do you think this kind of method would work for any serious NLP task ? (e.g. translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
